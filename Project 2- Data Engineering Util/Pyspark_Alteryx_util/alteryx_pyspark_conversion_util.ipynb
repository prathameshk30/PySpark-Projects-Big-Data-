{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_container_path_dict(relative_path: str,  container_type: str,  filename:str):\n",
    "    \"\"\"\n",
    "        This function creates the param dict for path that is consumed by function ->  get_adls_path_and_filename()\n",
    "        :param relative_path: common folder ,  container_type: silver / bronze\n",
    "        :return: dict of path parameters\n",
    "    \"\"\"\n",
    "    # Define the today's date\n",
    "    today = date.today()\n",
    "    formatted_date = today.strftime(\"%Y/%m/%d\")\n",
    "\n",
    "    # Define path & parameters\n",
    "    account_name_maindata   = f\"{mssparkutils.env.getWorkspaceName()[8:15]}2\"\n",
    "    account_name_metadata   = f\"{mssparkutils.env.getWorkspaceName()[8:15]}1\"\n",
    "\n",
    "    # Define the para dict\n",
    "    path_to_destination_dict = {\"account\":f\"adlsba{account_name_maindata}\",\n",
    "                                \"folder_path\":f\"{relative_path}\",\n",
    "                                \"filename\": f\"{filename}\",\n",
    "                                \"date\":f\"{formatted_date}\",\n",
    "                                \"container\": f\"{container_type}\"}\n",
    "    \n",
    "    return path_to_destination_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dates(start_date_str, end_date_str, number_of_days_between):\n",
    "    \"\"\"\n",
    "        function develops list of dates between given start date and end date\n",
    "        :param start_date_str: start dates,  end_date_str: end_dates, number_of_days_between: interval between gives dates\n",
    "        :return: list of dates\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "    dates_list = []\n",
    "\n",
    "    while start_date <= end_date:\n",
    "        # Add the current date to the list\n",
    "        dates_list.append(start_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "        # Move to the first day of the next quarter\n",
    "        start_date = start_date + timedelta(number_of_days_between)\n",
    "\n",
    "    return dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_in_directory(directory_path):\n",
    "    \"\"\"\n",
    "    List files details in the given directory.\n",
    "\n",
    "    Parameters:\n",
    "        directory_path (str): Path to the directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing file details.\n",
    "    \"\"\"\n",
    "    files = mssparkutils.fs.ls(directory_path)\n",
    "\n",
    "    file_name = []\n",
    "    is_dir = []\n",
    "    is_file = []\n",
    "    path = []\n",
    "    size = []\n",
    "\n",
    "    for file in files:\n",
    "        file_name.append(file.name)\n",
    "        is_dir.append(file.isDir)\n",
    "        is_file.append(file.isFile)\n",
    "        path.append(file.path)\n",
    "        size.append(file.size)\n",
    "\n",
    "    dir_dict = {\n",
    "        \"File Name\": file_name,\n",
    "        \"Is Directory\": is_dir,\n",
    "        \"Is File\": is_file,\n",
    "        \"Path\": path,\n",
    "        \"Size\": size\n",
    "    }\n",
    "\n",
    "    return dir_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shape(df):\n",
    "    \"\"\"\n",
    "    Return the shape of Dataframe\n",
    "    \"\"\"\n",
    "    return print(f\"\\n Dataframe Shape= Number of rows:{df.count()}, Number of columns:{len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure unique column names\n",
    "def make_unique(column_list):\n",
    "    counts = {}\n",
    "    result = []\n",
    "    for col in column_list:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            new_col = f\"{col}_{counts[col]}\"\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            new_col = col\n",
    "        result.append(new_col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alteryx to Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_union(left_df, right_df, left_col: str, right_col: str):\n",
    "    \"\"\"\n",
    "    This function takes two dataframes and then proceeds for a left and inner join on them. Then performs unions both of the resultant datasets.\n",
    "    \"\"\"\n",
    "    # Perform left join\n",
    "    left_join_df = left_df.join(right_df, left_col, right_col, \"left\")\n",
    "\n",
    "    # Perform inner join\n",
    "    inner_join_df = left_df.join(right_df, left_col, right_col, \"inner\")\n",
    "\n",
    "    # Union the resulting DataFrames\n",
    "    union_df = left_join_df.union(inner_join_df)\n",
    "\n",
    "    return union_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_keep_columns(DF_L, DF_R, left_key, right_key, left_key_2, right_key_2,  type_of_join, keep_columns):\n",
    "    \"\"\"\n",
    "    This function gives a DataFrame keeping only left/right DataFrame columns after joining two DataFrames, based on chosen DataFrame configuration.\n",
    "    \"\"\"\n",
    "    # Alias the columns in the left DataFrame (df1) before the join\n",
    "    df1_aliased = DF_L.alias(\"left\")\n",
    "\n",
    "    # Alias the columns in the right DataFrame (df2) before the join\n",
    "    df2_aliased = DF_R.alias(\"right\")\n",
    "\n",
    "    if left_key_2==None and right_key_2==None:\n",
    "        # Perform inner join between aliased DataFrames\n",
    "        joined_df = df1_aliased.join(df2_aliased, (df1_aliased[left_key] == df2_aliased[right_key]), type_of_join)\n",
    "    else:\n",
    "        # Perform inner join between aliased DataFrames\n",
    "        joined_df = df1_aliased.join(df2_aliased, (df1_aliased[left_key] == df2_aliased[right_key]) & (df1_aliased[left_key_2] == df2_aliased[right_key_2]), type_of_join)\n",
    "\n",
    "    # Select only the columns from the left DataFrame (df1) after inner join\n",
    "    if keep_columns == 'left':\n",
    "        result_df = joined_df.select(\"left.*\")\n",
    "        result_df = result_df.select([F.col(x).alias(x.replace('left.', '')) for x in result_df.columns])\n",
    "    else:\n",
    "        result_df = joined_df.select(\"right.*\")\n",
    "        result_df = result_df.select([F.col(x).alias(x.replace('right.', '')) for x in result_df.columns])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_select_columns(DF_L, DF_R, left_key, right_key, type_of_join, keep_columns_left, keep_columns_right):\n",
    "    \"\"\"\n",
    "    This function gives a DataFrame keeping only left/right DataFrame columns after joining two DataFrames, based on chosen DataFrame configuration.\n",
    "    \"\"\"\n",
    "    # Alias the columns in the left DataFrame (DF_L) before the join\n",
    "    df1_aliased = DF_L.alias(\"left\")\n",
    "\n",
    "    # Alias the columns in the right DataFrame (DF_R) before the join\n",
    "    df2_aliased = DF_R.alias(\"right\")\n",
    "\n",
    "    # Perform inner join between aliased DataFrames\n",
    "    join_condition = None\n",
    "    for left_key, right_key in zip(left_key, right_key):\n",
    "        condition = df1_aliased[left_key] == df2_aliased[right_key]\n",
    "        if join_condition is None:\n",
    "            join_condition = condition\n",
    "        else:\n",
    "            join_condition = join_condition & condition\n",
    "\n",
    "    joined_df = df1_aliased.join(df2_aliased, join_condition, type_of_join)\n",
    "\n",
    "    # Select only the specified columns from both DataFrames after inner join\n",
    "    if keep_columns_left and keep_columns_right:\n",
    "        left_cols = [col(f\"left.{column}\").alias(column) for column in keep_columns_left]\n",
    "        right_cols = [col(f\"right.{column}\").alias(column) for column in keep_columns_right]\n",
    "        result_df = joined_df.select(left_cols + right_cols)\n",
    "\n",
    "    elif keep_columns_left:\n",
    "        result_df = joined_df.select([col(f\"left.{column}\").alias(column) for column in keep_columns_left])\n",
    "    elif keep_columns_right:\n",
    "        result_df = joined_df.select([col(f\"right.{column}\").alias(column) for column in keep_columns_right])\n",
    "    else:\n",
    "        raise ValueError(\"At least one of keep_columns_left or keep_columns_right should be provided.\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column(spark_df, new_column_name, starting_element_column, comparison_column):\n",
    "    \"\"\"\n",
    "    This function creates a new column based on given condition.\n",
    "    \"\"\"\n",
    "    # Extract the starting value from the DataFrame\n",
    "    start_value = spark_df.select(starting_element_column).first()[starting_element_column]\n",
    "    \n",
    "    # Create a new DataFrame with the new column based on the condition\n",
    "    result_df = spark_df.withColumn(new_column_name,\n",
    "                                     when(col(comparison_column) < lit(start_value), col(starting_element_column))\n",
    "                                     .otherwise(expr(f\"date_add(`{starting_element_column}`, 1)\")))\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alteryx_transpose(spark_df, key_columns):\n",
    "    \"\"\"\n",
    "    This function replicates Alteryx transpose.\n",
    "    \"\"\"\n",
    "    df=spark_df\n",
    "    # Reshape DataFrame using melt and pivot\n",
    "    id_vars = key_columns\n",
    "    value_vars = [column for column in df.columns if column not in id_vars]\n",
    "\n",
    "    melted_df = df.select(id_vars + [\n",
    "        explode(array([\n",
    "            struct(lit(c).alias(\"Name\"), col(c).alias(\"Value\")) for c in value_vars\n",
    "        ])).alias(\"Agg_Value\")\n",
    "    ])\n",
    "\n",
    "    # # Extract \"Name\" and \"Value\" from \"Agg_Value\"\n",
    "    transposed_df = melted_df.select(\n",
    "        *[col(var).alias(var) for var in id_vars],  # Rename id_vars columns\n",
    "        col(\"Agg_Value\").getItem(\"Name\").alias(\"Name\"),\n",
    "        col(\"Agg_Value\").getItem(\"Value\").alias(\"Value\")\n",
    "    )\n",
    "\n",
    "    return transposed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alteryx_sample(spark_df, group_columns, select_method='first'):\n",
    "    \"\"\"\n",
    "    This function replicates the Alteryx sample function.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.window import Window\n",
    "    import pyspark.sql.functions as F\n",
    "\n",
    "    # Define a window specification partitioned by the group columns and ordered by the same columns\n",
    "    window_spec = Window.partitionBy(*group_columns).orderBy(F.lit(1))\n",
    "    # Add row numbers to each row within the partition\n",
    "    spark_df_with_row_number = spark_df.withColumn('row', F.row_number().over(window_spec))\n",
    "\n",
    "    if select_method == 'first':\n",
    "        # Get the first row for each group\n",
    "        record_df = spark_df_with_row_number.filter(F.col('row') == 1).drop('row')\n",
    "    elif select_method == 'last':\n",
    "        # Get the last row for each group\n",
    "        record_df = spark_df_with_row_number.selectExpr(\"*\", \"max(row) over (partition by `{}`) as max_row\".format(\"`, `\".join(group_columns))) \\\n",
    "                                                .filter((F.col('row') == F.col('max_row')) | (F.col('max_row').isNull())).drop('row', 'max_row')\n",
    "    else:\n",
    "        raise ValueError(\"Invalid select_method. Use 'first' or 'last'.\")\n",
    "\n",
    "    return record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alteryx_cross_tab(spark_df, group_columns, pivot_column, value_column, method_agg):\n",
    "    # Perform cross-tabulation\n",
    "    if method_agg==\"first\":\n",
    "        cross_tab_df = spark_df.groupBy(*group_columns).pivot(pivot_column).agg(first(value_column))\n",
    "    elif method_agg==\"last\":\n",
    "        cross_tab_df = spark_df.groupBy(*group_columns).pivot(pivot_column).agg(last(value_column))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid select_method. Use 'first' or 'last'.\")\n",
    "    return cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_columns_names(df_1, df_2):\n",
    "    \"\"\"\n",
    "    This function checks two column names (headers) for two DataFrames and adds \"None\" value for missing columns, \n",
    "    and arranges the columns based on df_1.\n",
    "    \"\"\"\n",
    "    # Find columns present in df_1 but not in df_2\n",
    "    columns_not_in_df_2 = [col for col in df_1.columns if col not in df_2.columns]\n",
    "    # Add missing columns with None values to df_2\n",
    "    for col in columns_not_in_df_2:\n",
    "        df_2 = df_2.withColumn(col, lit(None))\n",
    "    \n",
    "    # Find columns present in df_2 but not in df_1\n",
    "    columns_not_in_df_1 = [col for col in df_2.columns if col not in df_1.columns]\n",
    "    # Add missing columns with None values to df_1\n",
    "    for col in columns_not_in_df_1:\n",
    "        df_1 = df_1.withColumn(col, lit(None))\n",
    "    \n",
    "    # Arrange columns in df_2 to match the order of df_1\n",
    "    df_2 = df_2.select(*df_1.columns)\n",
    "    \n",
    "    return df_1, df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alteryx_unique(spark_df, column_name_list):\n",
    "    \"\"\"\n",
    "    This func creates a unique df similiar as alteryx.\n",
    "    \"\"\"\n",
    "    # Add a monotonically increasing ID column\n",
    "    df = spark_df.withColumn('row_id', F.monotonically_increasing_id())\n",
    "\n",
    "    # Define a window specification for ordering by the row_id\n",
    "    window_spec = Window.partitionBy(*column_name_list).orderBy('row_id')\n",
    "\n",
    "    # Assign row numbers based on the window specification\n",
    "    unique_df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    # Select only rows with row_num = 1 (first occurrence of each unique row)\n",
    "    unique_df = unique_df.filter('row_num = 1').drop('row_id', 'row_num')\n",
    "\n",
    "    return unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_sql_type_column_name(spark_df):\n",
    "    from pyspark.sql.functions import col\n",
    "    import re\n",
    "    '''\n",
    "    Assert SQL type column names by replacing spaces with underscores\n",
    "    '''\n",
    "    def clean_column_name(col_name):\n",
    "        # Replace any character that is not a-z, A-Z, 0-9, or underscore with an underscore\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9_]', '_', col_name.strip())\n",
    "        # Replace multiple underscores with a single underscore\n",
    "        clean_name = re.sub(r'_+', '_', clean_name)\n",
    "        return clean_name\n",
    "\n",
    "    # Use map to create a list of transformed columns\n",
    "    cols = list(map(lambda col_name: col(col_name).alias(clean_column_name(col_name)), spark_df.columns))\n",
    "    \n",
    "    # Select these columns from the DataFrame\n",
    "    spark_df = spark_df.select(*cols)\n",
    "    \n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCD 2 Merge in Delta Lake (Change tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import base64  # Ensure base64 is correctly imported\n",
    "\n",
    "def create_sql_hash_key(uid_str: str, encode=False, decode=False, encode_std=None):\n",
    "    \"\"\"\n",
    "    Creates a hash key using base64 encoding on a concatenated string.\n",
    "    \n",
    "    Parameters:\n",
    "    uid_str: The string to encode or decode.\n",
    "    encode: If True, the function will encode the string to base64.\n",
    "    decode: If True, the function will decode the base64 string back to its original form.\n",
    "    encode_std: utf-8/ ascii etc\n",
    "    \n",
    "    Returns:\n",
    "    The encoded or decoded string based on the flag provided.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if uid_str is not None and encode:\n",
    "            # ASCII encode (8 bit)\n",
    "            sample_string_bytes = uid_str.encode(encode_std)\n",
    "            # Convert byte string to base64 bytes\n",
    "            base64_bytes = base64.b64encode(sample_string_bytes)\n",
    "            # Decode the base64 bytes to a string\n",
    "            base64_string = base64_bytes.decode(encode_std)\n",
    "            return base64_string\n",
    "        elif uid_str is not None and decode:\n",
    "            # ASCII encode (8 bit)\n",
    "            base64_bytes = uid_str.encode(encode_std)\n",
    "            # Decode the base64 bytes back to the original string bytes\n",
    "            sample_string_bytes = base64.b64decode(base64_bytes)\n",
    "            # Decode the bytes to a string\n",
    "            sample_string = sample_string_bytes.decode(encode_std)\n",
    "            return sample_string\n",
    "        else:\n",
    "            raise ValueError(\"Check uid_str, encode, decode values or import python lib: base64\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error:\\n{e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scd2_merge_df(bronze_spark_df, silver_delta_table, id_col, hash_col):\n",
    "    \"\"\"\n",
    "    Merge the bronze DataFrame into the silver Delta table using SCD2 type for change tracking.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add change tracking columns to the bronze DataFrame\n",
    "    bronze_spark_df = bronze_spark_df.withColumn(\"start_date\", lit(None).cast(\"timestamp\")) \\\n",
    "                                     .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "                                     .withColumn(\"changed_column\", lit(None).cast(\"string\")) \\\n",
    "                                     .withColumn(\"record_flag\", lit(None).cast(\"string\"))\\\n",
    "                                     .withColumn(\"old_hash_col\", lit(None).cast('string'))\n",
    "    \n",
    "    # Define merge condition\n",
    "    merge_condition = f\"silver.{id_col} = bronze.{id_col} AND silver.{hash_col} = bronze.{hash_col}\"\n",
    "    \n",
    "    # Merge operation with SCD2 logic\n",
    "    silver_delta_table.alias(\"silver\").merge(\n",
    "        bronze_spark_df.alias(\"bronze\"),\n",
    "        merge_condition\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    # Convert the updated Delta table back to a Spark DataFrame\n",
    "    silver_spark_df = silver_delta_table.toDF()\n",
    "\n",
    "    # Drop old has column\n",
    "    silver_spark_df = silver_spark_df.drop(\"old_hash_col\")\n",
    "\n",
    "    # Change start date\n",
    "    # Update the start_date for records where record_flag is \"Current\"\n",
    "    silver_spark_df = silver_spark_df.withColumn( \"start_date\", \n",
    "                                                 when(col(\"record_flag\") == \"Current\", col(\"start_date\")\n",
    "                                                      ).otherwise(current_timestamp())\n",
    "                                                )\n",
    "    \n",
    "    # Define window for partitioning by ID with ordering\n",
    "    custom_window = Window.partitionBy(id_col).orderBy(\"start_date\")\n",
    "    \n",
    "    # Track changes using hash column\n",
    "    silver_spark_df = silver_spark_df.withColumn(\"old_hash_col\", lag(col(hash_col)).over(custom_window))\n",
    "\n",
    "    # Update end date\n",
    "    silver_spark_df = silver_spark_df.withColumn( \"end_date\", lead(col(\"start_date\")).over(custom_window))\n",
    "\n",
    "    # update record_flag\n",
    "    silver_spark_df = silver_spark_df.withColumn( \"record_flag\", \n",
    "                                                    when(col(\"end_date\").isNull(), \n",
    "                                                    lit(\"Current\")).otherwise(lit(\"Historical\"))\n",
    "                                                )\n",
    "\n",
    "    return silver_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_changed_columns_for_SCD2(hash_key_1: str, hash_key_2: str, delimiter: str, columns_list: list):\n",
    "    \"\"\"\n",
    "    This function compares two strings (hash_key_1 and hash_key_2), splits them using a delimiter,\n",
    "    and returns a list of columns whose values have changed between the two hash keys.\n",
    "    \"\"\"\n",
    "    values_list_1 = hash_key_1.split(delimiter)\n",
    "    values_list_2 = hash_key_2.split(delimiter)\n",
    "\n",
    "    # define empty obj\n",
    "    column_dict = {}\n",
    "    changed_columns = []\n",
    "\n",
    "    # run validation\n",
    "    if len(values_list_1) == len(values_list_2):\n",
    "        if len(values_list_1) == len(columns_list):\n",
    "            # Map values to columns\n",
    "            for values_1, values_2, col_name in zip(values_list_1, values_list_2, columns_list):\n",
    "                column_dict[col_name] = [values_1, values_2]\n",
    "\n",
    "            # Check for changed values\n",
    "            for col, val in column_dict.items():\n",
    "                if val[0] != val[1]:\n",
    "                    changed_columns.append(col)\n",
    "\n",
    "            return changed_columns\n",
    "        else:\n",
    "            raise ValueError(\"Hashed column values and column list don't match in length\")\n",
    "    else:\n",
    "        raise ValueError(\"The two hashed column values don't match in length\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
